import numpy as np
import sys, os
import numpy as np
sys.path.append(os.pardir)
from dataset.mnist import load_mnist
from common.gradient import numerical_gradient
import matplotlib.pyplot as plt
from PIL import Image
import pickle
from common.functions import *
from collections import OrderedDict
from common.layers import *
from common.multi_layer_net import MultiLayerNet


#梯度 更新 类
class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr

    def update(self, params, grad):
        for key in params.keys():
            params[key] -= self.lr * grad[key]

#通过指定优化的人如optimizer = SGD() 然后在神经网络中模块化调用SGD


#添加速度v

class Momentum:
    def __init__(self, lr = 0.01, momentum = 0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None

    def update(self, params, grad):
        if self.v is None:
            self.v = {}
            for key, val in params.items():
                self.v[key] = np.zeros_like(val)

        for key in params.keys():
            self.v[key] = self.momentum * self.v[key] - self.lr * grad[key]
            params[key] += self.v[key]

#添加h，调整学习率
class AdaGrad:
    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None

    def update(self, params, grad):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
        for key in params.keys():
            self.h[key] += grad[key] * grad[key]  #关于此处，h表示为梯度的平方和
            params[key] -= self.lr * grad[key] / np.sqrt(self.h[key] + 1e-7)

#Adam算法，融合了agagrad和nomentum两种算法
class Adam:
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.iter = 0
        self.m = None
        self.v = None

    def update(self, params, grads):
        if self.m is None:
            self.m, self.v = {}, {}
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)

        self.iter += 1
        lr_t = self.lr * np.sqrt(1.0 - self.beta2 ** self.iter) / (1.0 - self.beta1 ** self.iter)

        for key in params.keys():
            #m=m + (1-b1)*(梯度-m)
            #v=v + (1-b2)*(梯度的平方-v)
            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])
            self.v[key] += (1 - self.beta2) * (grads[key] ** 2 - self.v[key])
            #params = params - 学习率 * m / (v+1e-7)的开方
            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)


(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
train_size = x_train.shape[0]
batch_size = 128
max_iterations = 2000

optimizers = {}
optimizers['SGD'] = SGD()
optimizers['Momentum'] = Momentum()
optimizers['AdaGrad'] = AdaGrad()
optimizers['Adam'] = Adam()

network = {}
train_loss_list = {}
for key in optimizers.keys():
    network[key] = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100], output_size=10)
    train_loss_list[key] = []

for i in range(max_iterations):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    for key in optimizers.keys():
        grad = network[key].gradient(x_batch, t_batch)
        optimizers[key].update(network[key].params, grad)
        loss = network[key].loss(x_batch, t_batch)
        train_loss_list[key].append(loss)

    if i % 100 == 0:
        print('===============' + 'iteration' + str(i) + '=================')
        for key in optimizers.keys():
            loss = network[key].loss(x_batch, t_batch)
            print(key + ':' + str(loss))




