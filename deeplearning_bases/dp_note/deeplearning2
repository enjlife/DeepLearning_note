import sys, os

sys.path.append(os.pardir)
from dataset.mnist import load_mnist
from deeplearning_bases.common.gradient import numerical_gradient
from collections import OrderedDict
from deeplearning_bases.common.layers import *


# class MulLayer:
#     def __init__(self):
#         self.x = None
#         self.y = None
#
#     def forward(self, x, y):
#         self.x = x
#         self.y = y
#         out = x * y
#         return out
#
#     def backward(self, dout):
#         dx = dout * self.y
#         dy = dout * self.x
#         return dx, dy
#
# class AddLayer:
#     def __init__(self):
#         pass  #不需要内部存储变量，所以不需特意进行初始化
#
#     def forward(self, x, y):
#         return x + y
#
#     def backward(self, dout):
#         dx = dout * 1
#         dy = dout * 1
#         return dx, dy
#
#
# class Relu:
#     def __init__(self):
#         self.mask = None
#
#     def forward(self, x):
#         self.mask = (x<0)
#         out = x.copy()
#         out[self.mask] = 0
#         return out
#     def backward(self, dout):
#         dout[self.mask] = 0
#         dx = dout
#         return dx
#
#
# class Sigmoid:
#     def __init__(self):
#         self.out = None
#     def forward(self, x):
#         self.out = 1/(1+(np.exp(-x)))
#
#     def backward(self, dout):
#         dx = dout * self.out * (1.0 - self.out)
#         return dx
#
#
# class Affine:
#     def __init__(self, W, b):
#         self.W = W
#         self.b = b
#         self.X = None
#         self.dW = None
#         self.db = None
#
#     def forward(self, X):
#         self.X = X
#         Y = np.dot(X, self.W) + self.b
#         return Y
#     def backward(self, dout):
#         dX = dout * self.W.T
#         self.dW = self.X.T * dout
#         self.db = np.sum(dout, 0)
#         return dX
#
# class SoftmaxWithLoss:
#     def __init__(self):
#         pass
#     def forward(self, t, x):
#         self.t = t
#         self.y = softmax(x)
#         self.loss = cross_entropy_error(self.y, self.t)
#         return self.loss
#
#     def backword(self, out):
#         batch_size = self.t.shape[0]
#         dx = (self.y - self.t) / batch_size
#         return dx



#购买两个苹果
#初始化
# apple = 100
# apple_sum = 2
# tax = 1.1
#
# mul_apple_layer = MulLayer()
# mul_tax_layer = MulLayer()
#
# #forward
# apple_price = mul_apple_layer.forward(apple, apple_sum) #调用类函数，需要使用实例调用，否则需要输入self变量
# price = mul_tax_layer.forward(apple_price, tax)
# dprice = 1
# dapple_price, dtax  = mul_tax_layer.backward(dprice)
# dapple, dapple_sum  = mul_apple_layer.backward(dapple_price)
# print(price)
# print(dapple_price, dtax)
# print(dapple, dapple_sum)

#购买两个苹果和3个橘子
# apple = 100
# apple_sum = 2
# orange = 150
# orange_sum = 3
# tax = 1.1
# dprice = 1
#
# mul_apple_layer = MulLayer()
# mul_orange_layer = MulLayer()  #缺少一个括号，TypeError: forward() missing 1 required positional argument: 'y'
# add_apple_orange_layer = AddLayer()
# mul_tax_layer = MulLayer()
#
# apple_price = mul_apple_layer.forward(apple, apple_sum)
# orange_price = mul_orange_layer.forward(orange, orange_sum)
# apple_orange_price = add_apple_orange_layer.forward(apple_price, orange_price)
# price = mul_tax_layer.forward(apple_orange_price, tax)
# print(price)
#
# dapple_orange_price, dtax = mul_tax_layer.backward(dprice)
# dapple_price, dorange_price = add_apple_orange_layer.backward(dapple_orange_price)
# dorange, dorange_sum = mul_orange_layer.backward(dorange_price)
# dapple, dapple_sum = mul_apple_layer.backward(dapple_price)
# print(dapple_orange_price, dtax, dapple_orange_price, dorange, dapple)

#误差反向传播算法实现
class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):
        #初始化权重
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)
        #生成层
        self.layers = OrderedDict()
        self.layers['Affinel1'] = Affine(self.params['W1'], self.params['b1'])
        self.layers['Relu'] = Relu()
        self.layers['Affinel2'] = Affine(self.params['W2'], self.params['b2'])
        self.lastlayer = SoftmaxWithLoss()

    def predict(self, x):
        for layer in self.layers.values(): #values需要有括号
            x = layer.forward(x)
        return x

    def loss(self, x, t):
        y = self.predict(x)
        return self.lastlayer.forward(y, t)

    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1:
            t = np.argmax(t, axis=1)
        accuracy = np.sum(y==t) / float(t.shape[0])
        return accuracy

    #计算梯度
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)

        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])

        return grads

    def gradient(self, x, t):
        #forward
        self.loss(x, t)
        #backward
        dout = 1
        dout = self.lastlayer.backward(dout)
        layers = list(self.layers.values())
        layers.reverse() #将resverse直接放到上行后，为none
        for layer in layers:
            dout = layer.backward(dout)
        grads = {}
        grads['W1'] = self.layers['Affinel1'].dW
        grads['b1'] = self.layers['Affinel1'].db
        grads['W2'] = self.layers['Affinel2'].dW
        grads['b2'] = self.layers['Affinel2'].db

        return grads


#梯度确认，比较两种梯度计算方法的结果数据是否相同
# (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
# network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)
# x_batch = x_train[0:3]
# t_batch = t_train[0:3]
# grad_numerical = network.numerical_gradient(x_batch, t_batch)
# grad_backprop = network.gradient(x_batch, t_batch)
# for key in grad_numerical.keys():
#     diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))
#     print(key + ':' + str(diff)) #字符串信息
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)
iters_num = 1000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1
train_loss_list = []
train_acc_list = []
test_acc_list = []

# iter_per_epoch = max(train_size / batch_size, 1)
#
# for i in range(iters_num):
#     batch_mask = np.random.choice(train_size, batch_size)
#     x_batch = x_train[batch_mask]
#     t_batch = t_train[batch_mask]
#
#     grad = network.gradient(x_batch, t_batch)
#
#     for key in ('W1', 'b1', 'W2', 'b2'):
#         network.params[key] -= learning_rate * grad[key]
#
#     loss = network.loss(x_batch, t_batch)
#     train_loss_list.append(loss)
#
#     if i % iter_per_epoch:
#         train_acc = network.accuracy(x_train, t_train)
#         test_acc = network.accuracy(x_test, t_test)
#         train_acc_list.append(train_acc)
#         test_acc_list.append(test_acc)
#         print(train_acc, test_acc)

iter_per_epoch = max(train_size/batch_size, 1)

for i in range(iters_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    grad = network.gradient(x_batch, t_batch)

    #for key in grad.keys():
    for key in ('W1', 'b1', 'W2', 'b2'):
        #network.params[key] = network.params[key] - (learning_rate * grad[key])
        network.params[key] -= learning_rate * grad[key]
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
#
    if i % iter_per_epoch:
        # train_acc = network.accuracy(x_train, t_train)
        # test_acc = network.accuracy(x_test, t_test)
        train_acc = network.accuracy(x_train, t_train) #此处训练采用完整训练集做测试
        test_acc = network.accuracy(x_test, t_test)   #此处导致循环过短
        # train_loss_list.append(train_acc)
        # test_acc_list.append(test_acc)
        # print(train_acc, test_acc)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print(train_acc, test_acc)


    # if i % iter_per_epoch:
    #     train_acc = network.accuracy(x_train, t_train)
    #     test_acc = network.accuracy(x_test, t_test)
    #     train_acc_list.append(train_acc)
    #     test_acc_list.append(test_acc)
    #     print(train_acc, test_acc)











