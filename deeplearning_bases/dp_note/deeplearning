import sys, os
import numpy as np
sys.path.append(os.pardir)
from dataset.mnist import load_mnist
from deeplearning_bases.common.gradient import numerical_gradient
import matplotlib.pyplot as plt


# ch03--学习笔记

#图片预览
# def img_show(img):
#     pil_img = Image.fromarray(np.uint8(img)) #将numpy数组的图像数据转换为PIL的数据对象
#     pil_img.show()
# img = x_train[0]
# label = t_train[0]
# print(label)
# print(img.shape)
# img = img.reshape(28, 28)
# print(img.shape)
# img_show(img)
# def sigmoid(z):
#     return 1 / (1 + np.exp(-z))

#softmax函数--求出所占比例
# def softmax(a):
#     c = np.max(a)
#     exp_a = np.exp(a-c)
#     exp_sum = np.sum(exp_a)
#     return exp_a / exp_sum
#sigmoid函数
# def sigmoid(x):
#     return 1 / (1+np.exp(-x))

#数据准备
# def get_data():
#     (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True,  flatten=True, one_hot_label=False)
#     return x_train, t_train
#
# def init_network():
#     with open('sample_weight.pkl', 'rb') as f : #读取二进制文件，注意文件是否过大决定读取方式，此处读取不到该文件
#         network = pickle.load(f)
#     return network
# def predict(network, x):
#     W1, W2, W3 = network['W1'], network['W2'], network['W3']
#     b1, b2, b3 = network['b1'], network['b1'], network['b1']
#     z1 = np.dot(x, W1) + b1
#     a1 = sigmoid(z1)
#     z2 = np.dot(a1, W2) + b2
#     a2 = sigmoid(z2)
#     z3 = np.dot(a2, W3) + b3
#     y = softmax(z3)
#     return y
# x, t = get_data()
# network = init_network()
# accuracy_cnt = 0
# for i in range(len(x)):
#     y = predict(network, x)
#     p = np.argmax(y)
#     if p == t[i]:
#         accuracy_cnt += 1
# print('accuracy:'+ str(float(accuracy_cnt) / len(x)))

#批处理,加快运算速度
# x, t = get_data()
# network = init_network()
# batch_size = 100
# accuracy_cnt = 0
# for i in range(0, len(x), batch_size):
#     x_batch = x[i:i+batch_size]
#     y_batch = predict(network, x_batch)
#     p = np.argmax(y_batch, axis=1) #矩阵的第0维是列方向，第1维是行方向
#     accuracy_cnt += np.sum(p == t[i:i+batch_size])
# print('accuracy:'+ str(float(accuracy_cnt) / len(x)))


#第4章
#均方误差
# def mean_squared_error(y, t):
#     return 0.5 * np.sum((y-t)**2)
# t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])
# y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])
# print(mean_squared_error(y, t))

#交叉熵误差
# def cross_entropy_error(y, t):
#     delta = 1e-7
#     return -np.sum(t * np.log(y + delta))
# t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])
# y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])
# print(cross_entropy_error(y, t))

#随机抽取训练数据
# x_train, t_train = get_data()
# train_size = x_train.shape[0]
# batch_size = 10
# batch_mask = np.random.choice(train_size, batch_size)
# x_batch = x_train[batch_mask]
# t_batch = t_train[batch_mask]
# print(batch_mask)
# print(x_train)

#交叉熵误差，支持单个和批量数据,输入预测值和准确值
# def cross_entropy_error(y, t):
#     if y.ndim == 1:
#         t = t.reshape(1, t.size)
#         y = y.reshape(1, y.size)
#     batch_size = y.shape[0]
#     return -np.sum(t * np.log(y + 1e-7)) / batch_size

# #如果监督数据是标签形式，计算交叉熵误差,捕捉错误（100，）和（100，10）不能同时广播
# def cross_entrory_error(y, t):
#     if y.ndim ==1:
#         t = t.reshape(1, t.size)
#         y = y.reshape(1, y.size)
#     batch_size = y.shape[0]
# IndexError: arrays used as indices must be of integer (or boolean) type 该处t不能作为indices
#     return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)

    # convert one-hot-vector to answer-label-index vector
    if t.size == y.size:
        t = t.argmax(axis=1)

    batch_size = y.shape[0]
    return - np.sum(np.log(y[np.arange(batch_size), t])) / batch_size
#数值微分
# def function_1(x):
#     return 0.01*x**2 + 0.1*x
# # x = np.arange(0, 20, 0.1)
# # y = function_1(x)
# # plt.xlabel('x')
# # plt.ylabel('y')
# # plt.plot(x, y)
# # plt.show()
# def numerical_diff(f, x):
#     h = 1e-4
#     return (f(x+h) - f(x-h)) / (2*h)
# print(numerical_diff(function_1, 5))

#计算给定x的偏导数，例如（x1,x2,x3），会计算出三个偏导数
# def _numerical_gradient_nobatch(f, x):
#     h = 1e-4
#     grad = np.zeros_like(x)
#     for idx in range(x.size):
#         val = x[idx]
#         #f1(x)
#         x[idx] = val + h
#         fxh1 = f(x)
#         #f2(x)
#         x[idx] = val - h
#         fxh2 = f(x)
#
#         grad[idx] = (fxh1 - fxh2) / (2*h)
#         x[idx] = val
#     return grad
# #包装上述偏导计算函数，如果维度大于1维，通过enumerate每行取x for循环计算
# def numerical_gradient(f, X):
#     if X.ndim == 1:
#         return _numerical_gradient_nobatch(f, X)
#     else:
#         grad = np.zeros_like(X)
#
#         for idx, x in enumerate(X):
#            grad[idx] = _numerical_gradient_nobatch(f, x)
#
#         return grad
# def function_2(x):
#     return x[0]**2 + x[1]**2
#g = numerical_gradient(function_2, np.array([3.0, 4.0])) #没有加小数点，导致结果差距很大

#使用梯度下降法求最小值，计算下降100次后的结果
# def gradient_decent(f, init_x, lr = 0.01, step_num = 100):
#     x = init_x
#     for i in range(step_num):
#         grad = numerical_gradient(f, x)
#         x -=lr * grad
#     return x
# init_x = np.array([-3.0, 4.0])
# g = gradient_decent(function_2, init_x, lr=0.1, step_num=100)
# print(g)

#神经网络的类
# class simpleNet:
#     def __init__(self):
#         self.W = np.random.randn(2,3)
#     def predict(self, x):
#         return np.dot(x, self.W)
#     def loss(self, x, t):
#         z = self.predict(x)
#         y = softmax(z)
#         loss = cross_entrory_error(y, t)
#         return loss
# net = simpleNet()
# print(net.W)
# x = np.array([0.6, 0.9])
# p = net.predict(x)
# print(p)
# print(np.argmax(p))
# t = np.array([0, 0, 1])
# print(net.loss(x, t))
# #偏导计算函数的输入变量，函数+自变量权重
# f = lambda w : net.loss(x, t)
# dW = numerical_gradient(f, net.W)
# print(dW)
class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):
        # self.params = {}
        # self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        # self.params['b1'] = np.zeros_like(hidden_size) #此处不应该使用like，切记
        # self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        # self.params['b2'] = np.zeros_like(output_size)

        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)
    def predict(self, x):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        z1 = np.dot(x, W1) + b1
        a1 = sigmoid(z1)
        z2 = np.dot(a1, W2) + b2
        y  = sigmoid(z2)
        return y
    def loss(self, x, t):
        y = self.predict(x)
        return cross_entropy_error(y, t)
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)

        accuracy = (np.sum(y == t)) / float(x.shape[0])  #准确率=正确除以总的训练数据
        return accuracy
    #计算梯度下降矩阵
    def numerical(self, x, t):
        loss_W = lambda W : self.loss(x, t)
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])

        # loss_W = lambda W: self.loss(x, t)
        #
        # grads = {}
        # grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        # grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        # grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        # grads['b2'] = numerical_gradient(loss_W, self.params['b2'])

        return grads


#mini-batch的实现，获取数据,one_hot_labe会对监督数据做处理
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

train_loss_list = []    #误差的变化列表
train_acc_list = []
test_acc_list = []
train_size = x_train.shape[0]   #训练集大小
batch_size = 10   #mini-batch随机抽取的数量
iter_per_epoch = max(train_size / batch_size, 1) #平均每个epoch的重复次数
iter_num = 10      #循环次数
learning_rate = 0.1    #学习率
network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)
for i in range(iter_num):
    #随机选取100个数据
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    #计算梯度
    grad = network.numerical(x_batch, t_batch)

    #更新参数
    for key in ('W1','b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
        #network.params[key] -= learning_rate * grad[key]
    train_loss_list.append(network.loss(x_batch, t_batch))
    #计算每个epoch的识别精度
    if i % iter_per_epoch ==0:   #i达到600，余数为0
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print('train acc, test acc | ' + str(train_acc) + ',' + str(test_acc))

x = np.arange(len(train_loss_list))
plt.plot(x, train_loss_list)
plt.show()



















